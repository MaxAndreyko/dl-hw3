{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04fc7d7c",
   "metadata": {},
   "source": [
    "# 1. Elvish names generation\n",
    "\n",
    "Language model implemented with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regular-sequence",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "with open('elvish-names.pickle', \"rb\") as file:\n",
    "    names_data = pickle.load(file)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e943e15",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.choice(names_data, 10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "overall-luther",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "**Processing pipeline:**\n",
    "\n",
    "chars (tokens) $\\to$ indices $\\to$ embeddings $\\to$ RNN output, hidden $\\to$ next char logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "velvet-desktop",
   "metadata": {},
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "\n",
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, names):\n",
    "        self.names = names\n",
    "        self.specials = ['<pad>', '<bos>', '<eos>']\n",
    "        self.vocab = build_vocab_from_iterator(names, specials=self.specials)\n",
    "        self.itos = self.vocab.get_itos()\n",
    "        self.pad_index = self.vocab['<pad>']\n",
    "        self.bos_index = self.vocab['<bos>']\n",
    "        self.eos_index = self.vocab['<eos>']\n",
    "        self.max_len = max(len(name) for name in names) + 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def str_to_idx(self, chars):\n",
    "        return [self.vocab[char] for char in chars]\n",
    "    \n",
    "    def idx_to_str(self, idx):\n",
    "        return [self.itos[index] for index in idx]\n",
    "    \n",
    "    def encode(self, chars):\n",
    "        chars = ['<bos>'] + list(chars) + ['<eos>']\n",
    "        return self.str_to_idx(chars)\n",
    "        \n",
    "    def decode(self, idx):\n",
    "        chars = self.idx_to_str(idx)\n",
    "        return ''.join(char for char in chars if char not in self.specials)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        encoded = self.encode(self.names[item])\n",
    "        padded = torch.full((self.max_len, ), self.pad_index, dtype=torch.int64)\n",
    "        padded[:len(encoded)] = torch.tensor(encoded)\n",
    "        return padded, len(encoded)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "realistic-muslim",
   "metadata": {},
   "source": [
    "dataset = NamesDataset(names_data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cultural-substance",
   "metadata": {},
   "source": [
    "for names, lengths in dataloader:\n",
    "    break\n",
    "\n",
    "# 0 - <pad>\n",
    "# 1 - <bos>\n",
    "# 2 - <eos>\n",
    "names[:4], lengths[:4]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "centered-religious",
   "metadata": {},
   "source": [
    "## Generator model\n",
    "\n",
    "Embedding + RNN + Linear head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b27a44b0",
   "metadata": {},
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unexpected-palestinian",
   "metadata": {},
   "source": [
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, dataset, embed_dim=32, hidden_dim=32, max_len=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=len(dataset.vocab_size),\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=dataset.pad_index\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=embed_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_dim, len(dataset.vocab))\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        \n",
    "        self.max_len = dataset.max_len if max_len is None else max_len\n",
    "\n",
    "    def forward(self, tokens, lengths):\n",
    "        '''\n",
    "        B - batch size\n",
    "        L - sequence length\n",
    "        E - embedding dim\n",
    "        H - hidden dim\n",
    "        V - vocab size\n",
    "        '''\n",
    "        # tokens: (B, L)\n",
    "        embeds = self.embedding(tokens)\n",
    "        # embeds: (B, L, E) in padded form\n",
    "        packed_embeds = pack_padded_sequence(embeds, lengths, batch_first=True, enforce_sorted=False)\n",
    "        outputs, hidden = self.rnn(packed_embeds)\n",
    "        # output: (B, L, H), hidden: (B, H) in packed form\n",
    "        outputs, lengths = pad_packed_sequence(outputs, batch_first=True)\n",
    "        logits = self.linear(outputs)\n",
    "        # logits: (B, L, V)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference(self, prefix=''):\n",
    "        # encode prefix\n",
    "        tokens = self.dataset.encode(prefix)[:-1] # [bos, prefix]\n",
    "        tokens = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "\n",
    "        # generate hidden for prefix\n",
    "        embeds = self.embedding(tokens)\n",
    "        output, hidden = self.rnn(embeds)\n",
    "        logits = self.linear(output)\n",
    "\n",
    "        # sample new token from logits\n",
    "        new_tokens = Categorical(logits=logits[:, -1:]).sample()\n",
    "        tokens = torch.cat([tokens, new_tokens], dim=1)\n",
    "\n",
    "        # 2 stopping conditions: reaching max len or getting <eos> token\n",
    "        while tokens.shape[1] < self.max_len:\n",
    "            if new_tokens.item() == self.dataset.eos_index:\n",
    "                break\n",
    "\n",
    "            # process newly obtained token\n",
    "            embeds = self.embedding(new_tokens)\n",
    "            output, hidden = self.rnn(embeds, hidden)\n",
    "            logits = self.linear(output)\n",
    "            # sample the next token from logits\n",
    "            new_tokens = Categorical(logits=logits[:, -1:]).sample()\n",
    "            tokens = torch.cat([tokens, new_tokens], dim=1)\n",
    "\n",
    "        # decode result to a string\n",
    "        return self.dataset.decode(tokens.squeeze())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "super-retreat",
   "metadata": {},
   "source": [
    "model = Generator(dataset, embed_dim=40, hidden_dim=40).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.pad_index)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d667b2b-0e72-4891-842a-a8eb2b2282fc",
   "metadata": {},
   "source": [
    "tokens, lengths = next(iter(dataloader))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48a44d78-79ec-4c29-82e4-b8e29bf086d8",
   "metadata": {},
   "source": [
    "lengths"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84f84730-5280-4de5-9cae-bbd48757d6e9",
   "metadata": {},
   "source": [
    "tokens.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "317a9340-0f0b-4c48-a670-149974b00b33",
   "metadata": {},
   "source": [
    "embeds = model.embedding(tokens)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03d50129-d2f4-4375-a90d-539e6079d92e",
   "metadata": {},
   "source": [
    "embeds.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6bd93a69-4fd8-46e9-bd41-fdfa9558b9b4",
   "metadata": {},
   "source": [
    "packed_embeds = pack_padded_sequence(embeds, lengths, batch_first=True, enforce_sorted=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0fd466de-32bd-4646-ac90-fc10fb34788a",
   "metadata": {},
   "source": [
    "packed_embeds"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c7d22610-d1a1-4d6a-803c-254f3b44d59f",
   "metadata": {},
   "source": [
    "outputs, hidden = model.rnn(packed_embeds)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf83f59c-32a3-4848-bcd2-3755349b3e2a",
   "metadata": {},
   "source": [
    "outputs.data.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9419a1b-f52c-423f-97d9-24047d70a064",
   "metadata": {},
   "source": [
    "outputs, lengths = pad_packed_sequence(outputs, batch_first=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac6d8fa4-7f58-4bdd-8bed-493ba5f1f3f3",
   "metadata": {},
   "source": [
    "outputs.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c081256d-1cdd-45a9-af73-8fc04e752ab5",
   "metadata": {},
   "source": [
    "lengths"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "13ac4483-3906-4e47-91cd-800bc1795ebe",
   "metadata": {},
   "source": [
    "logits = model.linear(outputs)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "40364ecc-98cc-4bd6-895f-82d73fe7713e",
   "metadata": {},
   "source": [
    "logits.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "moved-environment",
   "metadata": {},
   "source": [
    "# inference for untrained model - trash\n",
    "[model.inference('') for _ in range(10)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "addressed-weight",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "sapphire-reference",
   "metadata": {},
   "source": [
    "def plot_losses(losses):\n",
    "    clear_output()\n",
    "    plt.plot(range(1, len(losses) + 1), losses)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "deluxe-delta",
   "metadata": {},
   "source": [
    "losses = []\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for tokens, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        tokens = tokens[:, :lengths.max()].to(device)\n",
    "        logits = model(tokens[:, :-1], lengths - 1)\n",
    "        # logits: (B, L, V), tokens: (B, L) - need to transpose logits\n",
    "        loss = criterion(logits.transpose(1, 2), tokens[:, 1:])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * tokens.shape[0]\n",
    "    \n",
    "    losses += [running_loss / len(dataset)]\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        plot_losses(losses)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "opponent-infrared",
   "metadata": {},
   "source": [
    "# unconditional generation\n",
    "[model.inference('') for _ in range(10)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "restricted-salmon",
   "metadata": {},
   "source": [
    "# conditional generation from prefix\n",
    "[model.inference('an') for _ in range(10)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cd0def9b",
   "metadata": {},
   "source": [
    "# 2. Image captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-ministry",
   "metadata": {},
   "source": [
    "%%bash\n",
    "kaggle datasets download -d adityajn105/flickr8k\n",
    "unzip flickr8k.zip"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "68ad0171",
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9abc9e16",
   "metadata": {},
   "source": [
    "captions = pd.read_csv('../datasets/flickr/captions.txt', sep=',').dropna()\n",
    "captions"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2236f7b4",
   "metadata": {},
   "source": [
    "image_file = captions.image.sample(1).iloc[0]\n",
    "\n",
    "for i, caption in enumerate(captions[captions.image == image_file].caption):\n",
    "    print(f'{i + 1}:', caption)\n",
    "\n",
    "Image.open(os.path.join('../datasets/flickr/Images', image_file))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "20388307",
   "metadata": {},
   "source": [
    "image_ids = {k: i for i, k in enumerate(captions.image.unique())}\n",
    "image_list = list(map(lambda x: x[0], sorted(image_ids.items(), key=lambda x: x[1])))\n",
    "captions['image_id'] = captions.image.map(image_ids)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a450df81",
   "metadata": {},
   "source": [
    "class ImagesDataset(Dataset):\n",
    "    def __init__(self, root, image_list, transform=None):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.image_list = image_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image_path = os.path.join(self.root, self.image_list[item])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5353b4bd",
   "metadata": {},
   "source": [
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "\n",
    "cnn_model = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1).eval().to(device)\n",
    "cnn_model.classifier = nn.Identity()\n",
    "\n",
    "cnn_model.eval()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1b43e54c",
   "metadata": {},
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = ImagesDataset(root='../datasets/flickr/Images/', image_list=image_list, transform=transform)\n",
    "dataloader = DataLoader(dataset, num_workers=4, shuffle=False, batch_size=128)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7142e1fc",
   "metadata": {},
   "source": [
    "image_embeds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in tqdm(dataloader):\n",
    "        embeds = cnn_model(images.to(device))\n",
    "        image_embeds += [embeds.cpu()]\n",
    "\n",
    "image_embeds = torch.cat(image_embeds, dim=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b202d6b6",
   "metadata": {},
   "source": [
    "i = 1001\n",
    "\n",
    "image = Image.open(f'../datasets/flickr/Images/{image_list[i]}').convert('RGB')\n",
    "image"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ceecb0a0",
   "metadata": {},
   "source": [
    "captions[captions.image_id == 1001]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7e57e913",
   "metadata": {},
   "source": [
    "image_embeds[i]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e0662e8",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    print(cnn_model(transform(image).to(device).unsqueeze(0)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "feff4283",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_images, test_images, train_embeds, test_embeds = \\\n",
    "    train_test_split(image_list, image_embeds, test_size=0.25, random_state=42)\n",
    "\n",
    "train_embeds.shape, test_embeds.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4b8c51ae",
   "metadata": {},
   "source": [
    "train_captions = captions[captions.image.isin(train_images)]\n",
    "test_captions = captions[captions.image.isin(test_images)]\n",
    "\n",
    "train_captions.shape, test_captions.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bbf761af",
   "metadata": {},
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
    "train_captions['tokens'] = train_captions.caption.apply(tokenizer)\n",
    "test_captions['tokens'] = test_captions.caption.apply(tokenizer)\n",
    "\n",
    "specials = ['<pad>', '<bos>', '<eos>', '<unk>']\n",
    "vocab = build_vocab_from_iterator(train_captions.tokens, specials=specials, min_freq=5)\n",
    "itos = vocab.get_itos()\n",
    "\n",
    "PAD_IDX, BOS_IDX, EOS_IDX, UNK_IDX = vocab['<pad>'], vocab['<bos>'], vocab['<eos>'], vocab['<unk>']\n",
    "vocab.set_default_index(UNK_IDX)\n",
    "VOCAB_SIZE = len(vocab)\n",
    "VOCAB_SIZE"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7666cfe6",
   "metadata": {},
   "source": [
    "print('Train max length:', train_captions.tokens.apply(len).max())\n",
    "plt.hist(train_captions.tokens.apply(len), bins=40)\n",
    "MAX_LENGTH = 30\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a453b9cb",
   "metadata": {},
   "source": [
    "train_captions"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0d3e6d52",
   "metadata": {},
   "source": [
    "class CaptionsDataset(Dataset):\n",
    "    def __init__(self, tokens, embeds):\n",
    "        super().__init__()\n",
    "        self.tokens = tokens\n",
    "        self.embeds = embeds\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def tokens_to_idx(self, tokens):\n",
    "        return [vocab[token] for token in tokens]\n",
    "    \n",
    "    def idx_to_tokens(self, idx):\n",
    "        return [itos[index] for index in idx]\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        tokens = ['<bos>'] + tokens + ['<eos>']\n",
    "        return self.tokens_to_idx(tokens)\n",
    "\n",
    "    def decode(self, idx):\n",
    "        tokens = self.idx_to_tokens(idx)\n",
    "        return ' '.join(token for token in tokens if token not in specials)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        tokens = self.tokens.iloc[item].tokens\n",
    "        encoded = self.encode(tokens)[:MAX_LENGTH + 2]\n",
    "        padded = torch.full((MAX_LENGTH + 2, ), PAD_IDX, dtype=torch.int64)\n",
    "        padded[:len(encoded)] = torch.tensor(encoded)\n",
    "\n",
    "        image_id = self.tokens.iloc[item].image_id\n",
    "        return padded, len(encoded), self.embeds[image_id]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a604e42a",
   "metadata": {},
   "source": [
    "train_set = CaptionsDataset(train_captions, image_embeds)\n",
    "test_set = CaptionsDataset(test_captions, image_embeds)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "# embeds_loader = DataLoader(test_embeds, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bce850ef",
   "metadata": {},
   "source": [
    "captions_list = []\n",
    "\n",
    "for image in tqdm(test_images):\n",
    "    data = test_captions[test_captions.image == image]\n",
    "    captions_list += [data.caption.str.lower().tolist()]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c397b7f1",
   "metadata": {},
   "source": [
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "\n",
    "\n",
    "class CaptionRNN(nn.Module):\n",
    "    def __init__(self, image_embed_dim, embed_dim=256, hidden_dim=256, lstm_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.cnn = MobileNet()\n",
    "        \n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.image_embed_to_h0 = nn.Sequential(\n",
    "            nn.Linear(image_embed_dim, lstm_layers * hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "        )\n",
    "        self.image_embed_to_c0 = nn.Sequential(\n",
    "            nn.Linear(image_embed_dim, lstm_layers * hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "        )\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=VOCAB_SIZE,\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=PAD_IDX\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, VOCAB_SIZE)\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens, lengths, image):\n",
    "        '''\n",
    "        B - batch size\n",
    "        M - lstm layers\n",
    "        L - sequence length\n",
    "        I - image embedding dim\n",
    "        E - embedding dim\n",
    "        H - hidden dim\n",
    "        V - vocab size\n",
    "        '''\n",
    "        image_embed = self.cnn(image)\n",
    "        \n",
    "        # image_embeds: (B, I)\n",
    "        B = image_embeds.shape[0]\n",
    "        h0 = self.image_embed_to_h0(image_embeds).reshape(B, self.lstm_layers, self.hidden_dim).transpose(0, 1)\n",
    "        c0 = self.image_embed_to_c0(image_embeds).reshape(B, self.lstm_layers, self.hidden_dim).transpose(0, 1)\n",
    "        # h0, co: (M, B, H)\n",
    "\n",
    "        # tokens: (B, L)\n",
    "        embeds = self.embedding(tokens)\n",
    "        # embeds: (B, L, E) in padded form\n",
    "\n",
    "        packed_embeds = pack_padded_sequence(embeds, lengths, batch_first=True, enforce_sorted=False)\n",
    "        outputs, (h, c) = self.lstm(packed_embeds, (h0, c0))\n",
    "        # output: (B, L, H) in packed form, (h, c): (M, B, H)\n",
    "        outputs, lengths = pad_packed_sequence(outputs, batch_first=True)\n",
    "        logits = self.linear(outputs)\n",
    "        # logits: (B, L, V)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference(self, image_embeds):\n",
    "        self.eval()\n",
    "        # generate lstm input\n",
    "        B = image_embeds.shape[0]\n",
    "        h = self.image_embed_to_h0(image_embeds).reshape(B, self.lstm_layers, self.hidden_dim).transpose(0, 1)\n",
    "        c = self.image_embed_to_c0(image_embeds).reshape(B, self.lstm_layers, self.hidden_dim).transpose(0, 1)\n",
    "        h, c = h.contiguous(), c.contiguous()\n",
    "\n",
    "        # init tokens with <bos>\n",
    "        tokens = torch.full((B, 1), BOS_IDX, device=image_embeds.device)\n",
    "        new_tokens = tokens\n",
    "\n",
    "        # 2 stopping conditions: reaching max len or getting <eos> token\n",
    "        while tokens.shape[1] < MAX_LENGTH:\n",
    "            if ((tokens == EOS_IDX).sum(1) > 0).all():\n",
    "                break\n",
    "\n",
    "            # process newly obtained token\n",
    "            embeds = self.embedding(new_tokens)\n",
    "            output, (h, c) = self.lstm(embeds, (h, c))\n",
    "            logits = self.linear(output)\n",
    "\n",
    "            # get new tokens from logits\n",
    "            new_tokens = logits.argmax(dim=-1)\n",
    "            tokens = torch.cat([tokens, new_tokens], dim=1)\n",
    "\n",
    "        return tokens"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c7915f93",
   "metadata": {},
   "source": [
    "# testing\n",
    "model = CaptionRNN(image_embeds.shape[1], lstm_layers=3)\n",
    "\n",
    "for tokens, lengths, embeds in train_loader:\n",
    "    break\n",
    "\n",
    "tokens = tokens[:, :lengths.max()]\n",
    "logits = model(tokens, lengths, embeds)\n",
    "assert logits.shape == (tokens.shape[0], tokens.shape[1], VOCAB_SIZE)\n",
    "\n",
    "tokens = model.inference(embeds)\n",
    "train_set.decode(tokens[0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cd4957b7",
   "metadata": {},
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "BLEU_FREQ = 5\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, test_losses, test_blues):\n",
    "    clear_output()\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(13, 4))\n",
    "    axs[0].plot(range(1, len(train_losses) + 1), train_losses, label='train', color='deepskyblue', linewidth=2)\n",
    "    axs[0].plot(range(1, len(test_losses) + 1), test_losses, label='test', color='springgreen', linewidth=2)\n",
    "    axs[0].set_ylabel('loss')\n",
    "\n",
    "    axs[1].plot(BLEU_FREQ * np.arange(1, len(test_blues) + 1), test_blues, label='test',\n",
    "                color='springgreen', linewidth=2)\n",
    "    axs[1].set_ylabel('BLEU')\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.legend()\n",
    "\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "34b87d56",
   "metadata": {},
   "source": [
    "from torchmetrics import BLEUScore\n",
    "\n",
    "\n",
    "def training_epoch(model, optimizer, criterion, train_loader, tqdm_desc):\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for tokens, lengths, embeds in tqdm(train_loader, desc=tqdm_desc):\n",
    "        embeds = embeds.to(device)\n",
    "        tokens = tokens[:, :lengths.max()].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(tokens[:, :-1], lengths - 1, embeds)\n",
    "        loss = criterion(logits.transpose(1, 2), tokens[:, 1:])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * embeds.shape[0]\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_epoch(model, criterion, valid_loader, tqdm_desc):\n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    for tokens, lengths, embeds in tqdm(valid_loader, desc=tqdm_desc):\n",
    "        embeds = embeds.to(device)\n",
    "        tokens = tokens[:, :lengths.max()].to(device)\n",
    "\n",
    "        logits = model(tokens[:, :-1], lengths - 1, embeds)\n",
    "        loss = criterion(logits.transpose(1, 2), tokens[:, 1:])\n",
    "\n",
    "        valid_loss += loss.item() * embeds.shape[0]\n",
    "\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    return valid_loss\n",
    "\n",
    "\n",
    "def evaluate_bleu(model, embeds_loader, captions):\n",
    "    bleu = BLEUScore()\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    for _, _, embeds in tqdm(embeds_loader, desc='Evaluating BLEU'):\n",
    "        tokens = model.inference(embeds.to(device)).cpu()\n",
    "        for token_list in tokens:\n",
    "            predictions += [train_set.decode(token_list)]\n",
    "    \n",
    "    return bleu(predictions, captions).item()\n",
    "\n",
    "\n",
    "def train(model, optimizer, scheduler, criterion, train_loader, valid_loader, captions_list, num_epochs):\n",
    "    train_losses, valid_losses, valid_blues = [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = training_epoch(\n",
    "            model, optimizer, criterion, train_loader,\n",
    "            tqdm_desc=f'Training {epoch}/{num_epochs}'\n",
    "        )\n",
    "        valid_loss = validation_epoch(\n",
    "            model, criterion, valid_loader,\n",
    "            tqdm_desc=f'Validating {epoch}/{num_epochs}'\n",
    "        )\n",
    "\n",
    "        if epoch % BLEU_FREQ == 0:\n",
    "            valid_bleu = evaluate_bleu(model, test_loader, captions_list)\n",
    "            valid_blues += [valid_bleu]\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_losses += [train_loss]\n",
    "        valid_losses += [valid_loss]\n",
    "        plot_losses(train_losses, valid_losses, valid_blues)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3b8a4b28",
   "metadata": {},
   "source": [
    "NUM_EPOCHS = 20\n",
    "\n",
    "model = CaptionRNN(image_embeds.shape[1], dropout=0.3).to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    {\n",
    "        \"params\": model.rnn.params, \"lr\": 1e-3, weight_decay=1e-6}\n",
    "    {\n",
    "        \"params\": model.cnn.params, \"lr\": 1e-4,\n",
    ")\n",
    "scheduler = None\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "sum(param.numel() for param in model.parameters())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bdccc648",
   "metadata": {},
   "source": [
    "print(f'Random BLEU: {evaluate_bleu(model, test_loader, captions_list):.4f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7b1f1905",
   "metadata": {},
   "source": [
    "train(model, optimizer, scheduler, criterion, train_loader, test_loader, captions_list, NUM_EPOCHS)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a6387b3e",
   "metadata": {},
   "source": [
    "def caption_random_test_image():\n",
    "    index = np.random.randint(len(test_images))\n",
    "    image_file = test_images[index]\n",
    "    tokens = model.inference(image_embeds[index].unsqueeze(0).to(device)).cpu().squeeze()\n",
    "    prediction = train_set.decode(tokens)\n",
    "    print('Prediction:', prediction)\n",
    "\n",
    "    for i, caption in enumerate(captions[captions.image == image_file].caption):\n",
    "        print(f'GT caption #{i + 1}:', caption)\n",
    "\n",
    "    return Image.open(os.path.join('../datasets/flickr/Images/', image_file)).convert('RGB')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0445658f",
   "metadata": {},
   "source": [
    "caption_random_test_image()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b7de79b2",
   "metadata": {},
   "source": [
    "# 3. Code reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a45ff5d4",
   "metadata": {},
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f2ba1a89",
   "metadata": {},
   "source": [
    "set_random_seed(0xDEADF00D)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8976a32d",
   "metadata": {},
   "source": [
    "a = nn.Conv2d(1, 1, kernel_size=3, bias=True).to(device)\n",
    "a.weight, a.bias"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e74d5907",
   "metadata": {},
   "source": [
    "x = torch.randn(1, 1, 5, 5).to(device)\n",
    "x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9f4fcef7",
   "metadata": {},
   "source": [
    "a(x)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5fb0851d",
   "metadata": {},
   "source": [
    "dataloader = DataLoader(torch.arange(20), shuffle=True, num_workers=4, batch_size=3)\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "eb8e21a6",
   "metadata": {},
   "source": [
    "set_random_seed(0xDEADF00D)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a5c8ab56",
   "metadata": {},
   "source": [
    "a = nn.Conv2d(1, 1, kernel_size=3, bias=True).to(device)\n",
    "a.weight, a.bias"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a6fb5eea",
   "metadata": {},
   "source": [
    "x = torch.randn(1, 1, 5, 5).to(device)\n",
    "x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0bf03995",
   "metadata": {},
   "source": [
    "a(x)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0c4a527a",
   "metadata": {},
   "source": [
    "dataloader = DataLoader(torch.arange(20), shuffle=True, num_workers=4, batch_size=3)\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5694d",
   "metadata": {},
   "source": [
    "def main():\n",
    "    set_seed()\n",
    "\n",
    "    model = Model()\n",
    "\n",
    "    dataloader = DataLoader()\n",
    "\n",
    "    for epoch in num_epoches:\n",
    "        train()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
